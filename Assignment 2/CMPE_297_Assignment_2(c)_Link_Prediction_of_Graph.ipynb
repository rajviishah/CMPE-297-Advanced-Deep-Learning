{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Link Prediction of Graph**"
      ],
      "metadata": {
        "id": "SYwYIRH5Axor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installation**"
      ],
      "metadata": {
        "id": "MY2TW8GPE5Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N29LNidx5YZ",
        "outputId": "1db865f5-a5f1-4ee4-8859-ce3e53b67943"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=db619c0c13f7c0c40ba94896ff345d187f3715fd8a5fb1c7c2fca249bea526ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNmwi5ldyEso",
        "outputId": "60431754-4c28-4c72-ae41-68c0494c1938"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_sparse\n",
            "  Downloading torch_sparse-0.6.15.tar.gz (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch_sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl size=516860 sha256=20596c6cd4d54a2ace5805eb9404445b9a8928b0e2f014f2f149ebee7a85fa7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/68/4d/1414be5c2c622bad35364e13213180797717b6d4b8923936dc\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdgaFd3H2tb2",
        "outputId": "9bcbb90b-b23b-47ac-f5cf-69f5d4bf1769"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=274491 sha256=c961cc627e4da3ff97894be7394dcd58a8e76c496acff5156feda7b7c2e8da4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import negative_sampling"
      ],
      "metadata": {
        "id": "cgS17-QU4XZ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DFjVOmUB4dW3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Transformation**"
      ],
      "metadata": {
        "id": "lumgumHgE-5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
        "                      add_negative_train_samples=False),\n",
        "])"
      ],
      "metadata": {
        "id": "TKXw1D9o4e76"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'Cora'\n",
        "path = osp.join('.', 'data', dataset)\n",
        "#path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
        "dataset = Planetoid(path, name='Cora', transform=transform)"
      ],
      "metadata": {
        "id": "a4LU8OBy4hLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c71f42-06a0-4540-c1c8-e633adc47d87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
        "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
        "# each element representing the corresponding split.\n",
        "train_data, val_data, test_data = dataset[0]"
      ],
      "metadata": {
        "id": "KnCYSZ1Y49xH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Modeling**"
      ],
      "metadata": {
        "id": "ka4lg_vJFIWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_label_index):\n",
        "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z):\n",
        "        prob_adj = z @ z.t()\n",
        "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
      ],
      "metadata": {
        "id": "YIxW4OgB5A0F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(dataset.num_features, 128, 64).to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "4MzN6D335B0S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(train_data.x, train_data.edge_index)\n",
        "\n",
        "    # We perform a new round of negative sampling for every training epoch:\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
        "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
        "\n",
        "    edge_label_index = torch.cat(\n",
        "        [train_data.edge_label_index, neg_edge_index],\n",
        "        dim=-1,\n",
        "    )\n",
        "    edge_label = torch.cat([\n",
        "        train_data.edge_label,\n",
        "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
        "    ], dim=0)\n",
        "\n",
        "    out = model.decode(z, edge_label_index).view(-1)\n",
        "    loss = criterion(out, edge_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wVHtaLXC5FTt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "lu4JB9cGFM3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
        "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
        "\n",
        "\n",
        "best_val_auc = final_test_auc = 0\n",
        "for epoch in range(1, 101):\n",
        "    loss = train()\n",
        "    val_auc = test(val_data)\n",
        "    test_auc = test(test_data)\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        final_test_auc = test_auc\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
        "          f'Test: {test_auc:.4f}')"
      ],
      "metadata": {
        "id": "MiEniC1J5F8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5146fce-dfe9-4af1-d10a-50cb4fd669a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.6930, Val: 0.6896, Test: 0.6975\n",
            "Epoch: 002, Loss: 0.6832, Val: 0.6812, Test: 0.6901\n",
            "Epoch: 003, Loss: 0.7077, Val: 0.6866, Test: 0.6992\n",
            "Epoch: 004, Loss: 0.6777, Val: 0.7101, Test: 0.7213\n",
            "Epoch: 005, Loss: 0.6853, Val: 0.7336, Test: 0.7411\n",
            "Epoch: 006, Loss: 0.6883, Val: 0.7444, Test: 0.7522\n",
            "Epoch: 007, Loss: 0.6890, Val: 0.7400, Test: 0.7482\n",
            "Epoch: 008, Loss: 0.6881, Val: 0.7235, Test: 0.7340\n",
            "Epoch: 009, Loss: 0.6849, Val: 0.7044, Test: 0.7196\n",
            "Epoch: 010, Loss: 0.6794, Val: 0.6911, Test: 0.7105\n",
            "Epoch: 011, Loss: 0.6730, Val: 0.6859, Test: 0.7057\n",
            "Epoch: 012, Loss: 0.6766, Val: 0.6879, Test: 0.7056\n",
            "Epoch: 013, Loss: 0.6735, Val: 0.6921, Test: 0.7068\n",
            "Epoch: 014, Loss: 0.6649, Val: 0.6994, Test: 0.7107\n",
            "Epoch: 015, Loss: 0.6622, Val: 0.7101, Test: 0.7161\n",
            "Epoch: 016, Loss: 0.6592, Val: 0.7181, Test: 0.7196\n",
            "Epoch: 017, Loss: 0.6548, Val: 0.7173, Test: 0.7181\n",
            "Epoch: 018, Loss: 0.6462, Val: 0.7147, Test: 0.7154\n",
            "Epoch: 019, Loss: 0.6383, Val: 0.7276, Test: 0.7225\n",
            "Epoch: 020, Loss: 0.6321, Val: 0.7630, Test: 0.7503\n",
            "Epoch: 021, Loss: 0.6200, Val: 0.7966, Test: 0.7827\n",
            "Epoch: 022, Loss: 0.6080, Val: 0.8012, Test: 0.7907\n",
            "Epoch: 023, Loss: 0.5974, Val: 0.7984, Test: 0.7903\n",
            "Epoch: 024, Loss: 0.5816, Val: 0.7965, Test: 0.7894\n",
            "Epoch: 025, Loss: 0.5659, Val: 0.7971, Test: 0.7928\n",
            "Epoch: 026, Loss: 0.5565, Val: 0.7955, Test: 0.7903\n",
            "Epoch: 027, Loss: 0.5524, Val: 0.7980, Test: 0.7914\n",
            "Epoch: 028, Loss: 0.5395, Val: 0.8077, Test: 0.8033\n",
            "Epoch: 029, Loss: 0.5297, Val: 0.8168, Test: 0.8108\n",
            "Epoch: 030, Loss: 0.5295, Val: 0.8218, Test: 0.8101\n",
            "Epoch: 031, Loss: 0.5318, Val: 0.8246, Test: 0.8121\n",
            "Epoch: 032, Loss: 0.5108, Val: 0.8260, Test: 0.8183\n",
            "Epoch: 033, Loss: 0.5154, Val: 0.8301, Test: 0.8234\n",
            "Epoch: 034, Loss: 0.5248, Val: 0.8296, Test: 0.8206\n",
            "Epoch: 035, Loss: 0.5069, Val: 0.8282, Test: 0.8189\n",
            "Epoch: 036, Loss: 0.5169, Val: 0.8325, Test: 0.8246\n",
            "Epoch: 037, Loss: 0.5165, Val: 0.8370, Test: 0.8319\n",
            "Epoch: 038, Loss: 0.5118, Val: 0.8396, Test: 0.8360\n",
            "Epoch: 039, Loss: 0.5090, Val: 0.8434, Test: 0.8393\n",
            "Epoch: 040, Loss: 0.5108, Val: 0.8461, Test: 0.8402\n",
            "Epoch: 041, Loss: 0.5026, Val: 0.8473, Test: 0.8394\n",
            "Epoch: 042, Loss: 0.5028, Val: 0.8494, Test: 0.8431\n",
            "Epoch: 043, Loss: 0.4945, Val: 0.8510, Test: 0.8470\n",
            "Epoch: 044, Loss: 0.4941, Val: 0.8515, Test: 0.8479\n",
            "Epoch: 045, Loss: 0.4913, Val: 0.8533, Test: 0.8474\n",
            "Epoch: 046, Loss: 0.4907, Val: 0.8548, Test: 0.8467\n",
            "Epoch: 047, Loss: 0.4888, Val: 0.8586, Test: 0.8515\n",
            "Epoch: 048, Loss: 0.4906, Val: 0.8620, Test: 0.8576\n",
            "Epoch: 049, Loss: 0.4894, Val: 0.8657, Test: 0.8612\n",
            "Epoch: 050, Loss: 0.4869, Val: 0.8701, Test: 0.8642\n",
            "Epoch: 051, Loss: 0.4801, Val: 0.8727, Test: 0.8658\n",
            "Epoch: 052, Loss: 0.4710, Val: 0.8740, Test: 0.8692\n",
            "Epoch: 053, Loss: 0.4736, Val: 0.8752, Test: 0.8731\n",
            "Epoch: 054, Loss: 0.4664, Val: 0.8762, Test: 0.8755\n",
            "Epoch: 055, Loss: 0.4741, Val: 0.8795, Test: 0.8762\n",
            "Epoch: 056, Loss: 0.4721, Val: 0.8804, Test: 0.8755\n",
            "Epoch: 057, Loss: 0.4782, Val: 0.8793, Test: 0.8760\n",
            "Epoch: 058, Loss: 0.4723, Val: 0.8784, Test: 0.8770\n",
            "Epoch: 059, Loss: 0.4759, Val: 0.8800, Test: 0.8764\n",
            "Epoch: 060, Loss: 0.4687, Val: 0.8817, Test: 0.8784\n",
            "Epoch: 061, Loss: 0.4652, Val: 0.8829, Test: 0.8812\n",
            "Epoch: 062, Loss: 0.4707, Val: 0.8837, Test: 0.8829\n",
            "Epoch: 063, Loss: 0.4663, Val: 0.8843, Test: 0.8833\n",
            "Epoch: 064, Loss: 0.4711, Val: 0.8859, Test: 0.8840\n",
            "Epoch: 065, Loss: 0.4716, Val: 0.8867, Test: 0.8857\n",
            "Epoch: 066, Loss: 0.4620, Val: 0.8879, Test: 0.8886\n",
            "Epoch: 067, Loss: 0.4729, Val: 0.8898, Test: 0.8904\n",
            "Epoch: 068, Loss: 0.4645, Val: 0.8886, Test: 0.8903\n",
            "Epoch: 069, Loss: 0.4581, Val: 0.8899, Test: 0.8903\n",
            "Epoch: 070, Loss: 0.4640, Val: 0.8924, Test: 0.8893\n",
            "Epoch: 071, Loss: 0.4596, Val: 0.8948, Test: 0.8891\n",
            "Epoch: 072, Loss: 0.4577, Val: 0.8965, Test: 0.8909\n",
            "Epoch: 073, Loss: 0.4593, Val: 0.8972, Test: 0.8938\n",
            "Epoch: 074, Loss: 0.4584, Val: 0.8961, Test: 0.8946\n",
            "Epoch: 075, Loss: 0.4615, Val: 0.8983, Test: 0.8944\n",
            "Epoch: 076, Loss: 0.4627, Val: 0.9001, Test: 0.8930\n",
            "Epoch: 077, Loss: 0.4551, Val: 0.9023, Test: 0.8942\n",
            "Epoch: 078, Loss: 0.4537, Val: 0.9033, Test: 0.8977\n",
            "Epoch: 079, Loss: 0.4614, Val: 0.9028, Test: 0.8995\n",
            "Epoch: 080, Loss: 0.4586, Val: 0.9044, Test: 0.8994\n",
            "Epoch: 081, Loss: 0.4557, Val: 0.9052, Test: 0.8980\n",
            "Epoch: 082, Loss: 0.4602, Val: 0.9068, Test: 0.9002\n",
            "Epoch: 083, Loss: 0.4546, Val: 0.9088, Test: 0.9034\n",
            "Epoch: 084, Loss: 0.4457, Val: 0.9085, Test: 0.9042\n",
            "Epoch: 085, Loss: 0.4488, Val: 0.9104, Test: 0.9045\n",
            "Epoch: 086, Loss: 0.4537, Val: 0.9101, Test: 0.9026\n",
            "Epoch: 087, Loss: 0.4532, Val: 0.9105, Test: 0.9026\n",
            "Epoch: 088, Loss: 0.4495, Val: 0.9108, Test: 0.9045\n",
            "Epoch: 089, Loss: 0.4497, Val: 0.9106, Test: 0.9048\n",
            "Epoch: 090, Loss: 0.4414, Val: 0.9112, Test: 0.9049\n",
            "Epoch: 091, Loss: 0.4459, Val: 0.9115, Test: 0.9041\n",
            "Epoch: 092, Loss: 0.4453, Val: 0.9113, Test: 0.9023\n",
            "Epoch: 093, Loss: 0.4451, Val: 0.9126, Test: 0.9028\n",
            "Epoch: 094, Loss: 0.4416, Val: 0.9137, Test: 0.9039\n",
            "Epoch: 095, Loss: 0.4402, Val: 0.9136, Test: 0.9050\n",
            "Epoch: 096, Loss: 0.4392, Val: 0.9140, Test: 0.9056\n",
            "Epoch: 097, Loss: 0.4430, Val: 0.9158, Test: 0.9047\n",
            "Epoch: 098, Loss: 0.4349, Val: 0.9171, Test: 0.9043\n",
            "Epoch: 099, Loss: 0.4411, Val: 0.9177, Test: 0.9047\n",
            "Epoch: 100, Loss: 0.4361, Val: 0.9168, Test: 0.9053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsCOxYhAxydz",
        "outputId": "d4a6130b-927e-4b7b-c931-7aa77a842d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test: 0.9047\n"
          ]
        }
      ],
      "source": [
        "print(f'Final Test: {final_test_auc:.4f}')\n",
        "\n",
        "z = model.encode(test_data.x, test_data.edge_index)\n",
        "final_edge_index = model.decode_all(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**\n",
        "\n",
        "\n",
        "\n",
        "*   https://github.com/pyg-team/pytorch_geometric/blob/master/examples/link_pred.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4bv4rPF-H3B"
      }
    }
  ]
}